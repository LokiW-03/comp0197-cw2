 Open-Ended Weakly-Supervised Semantic Segmentation Project

## 1. Overview

This project investigates the effectiveness of different **weak supervision** strategies for training semantic segmentation models. Specifically, it explores how various types and combinations of sparse, weakly-annotated labels impact model performance compared to traditional approaches like using only image-level labels or full pixel-level supervision.

The core focus is on using the **Oxford-IIIT Pet Dataset** and training a segmentation model (implicitly a SegNet variant, based on run names) using simulated weak annotations:

*   **Bounding Boxes:** Rectangular boxes encompassing the object of interest.
*   **Points:** A small number of points sampled within the object boundaries.
*   **Scribbles:** Short, free-form lines drawn roughly within the object boundaries.

The project examines the performance when using these annotation types individually and in various hybrid combinations.

## 2. Research Question

The central research question guiding this project is:

> How does the type and potential combination of sparse weak annotations (specifically bounding boxes, simulated points, and simulated scribbles) influence the segmentation performance and learning characteristics of a weakly-supervised model trained on the Oxford-IIIT Pet Dataset, compared to using only image-level labels or full pixel-level supervision?

This involves understanding:
*   The relative effectiveness of bounding boxes, points, and scribbles as standalone weak supervision signals.
*   Whether combining different types of weak annotations (hybrid supervision) yields better performance than single types.
*   How models trained with these weak signals compare (implicitly) to benchmarks trained with full masks or potentially just image tags.


## 3. Methodology

**Phase 1: Setup & Data Preparation**


1.  **Weak Label Generation:** 
    *   Points (centroid of each object mask).
    *   Scribbles (freehand stroke per object and background)
    *   Bounding boxes (tightest box around mask).

The process of generating weak labels begins by iterating through a specified subset of image files and locating their corresponding trimap annotations. For each trimap, the code first loads it, records its original dimensions, and converts it into a binary foreground mask by identifying pixels assumed to represent the object (checking for a specific pixel value, like 1 or 255, which needs to match the trimap's convention). If a valid foreground mask is created, several types of weak labels are derived from it using the original image coordinates: 

- Points are generated by finding distinct connected components (objects) within the mask and randomly sampling a predefined number of pixel coordinates (x, y) from within each component. 
- Scribbles are created by first finding the morphological skeleton (a thin line representation) of the foreground mask, identifying the longest path along this skeleton, and sampling points from it for the foreground scribble; background scribbles are similarly generated from the longest skeleton path within the region outside a dilated version of the foreground mask. 
- Bounding Boxes are determined by identifying each separate foreground object and calculating the minimum and maximum x and y coordinates that enclose it (xmin, ymin, xmax, ymax). 

These generated points, scribbles, and bounding boxes, along with the original image size, are then stored together, in a dictionary keyed by the image name, and saved to a file for later use in training weakly supervised models.
    

**Phase 2: Model Training**

*   **Data Preparation:**
    *   The `PetsDataset` is configured based on the chosen `supervision_mode` (e.g., 'points', 'scribbles', 'boxes', 'hybrid\_...').
    *   For each training sample, the dataloader provides:
        1.  The input image.
        2.  The corresponding *weak labels* (e.g., point coordinates, scribble paths, bounding box coordinates, or a mix in hybrid modes).
        3.  The *full ground truth (GT) segmentation mask* (primarily used for evaluation, not the weak training loss itself).

*   **Model Prediction:**
    *   The segmentation model (`SegNetWrapper`) processes the input image batch.
    *   It outputs dense, pixel-wise predictions (logits) representing the probability of each pixel belonging to the foreground (pet) or background class.

*   **Weakly Supervised Loss Calculation:**
    *   This is the key step where weak labels are used for learning.
    *   A specialized loss function (`CombinedLoss` or `PartialCrossEntropyLoss`) is employed.
    *   This loss function compares the model's predictions *only* at the locations specified by the *weak labels*.
        *   For **points/scribbles**: The loss is calculated based on whether the model correctly predicts foreground/background *at those specific pixel coordinates*. Pixels *not* covered by the points/scribbles are typically ignored in the loss calculation (e.g., using `IGNORE_INDEX`).
        *   For **boxes**: The loss might encourage the model to predict foreground *within* the box and potentially background *outside* it, or use other box-based weak supervision techniques integrated into the `CombinedLoss`.
        *   For **hybrid modes**: The `CombinedLoss` intelligently aggregates the loss signals derived from *all available* weak label types (points, scribbles, boxes) for that specific training image.
    *   The goal is to penalize the model for incorrect predictions *at the weakly labeled locations*.

*   **Model Update:**
    *   The calculated loss value, derived *from the weak supervision signal*, is used for backpropagation.
    *   Gradients are computed, and the optimizer (`AdamW`) updates the model's weights to minimize this weak loss over time.

*   **Evaluation and Checkpointing:**
    *   **Crucially:** While training relies on the *weak loss*, the model's actual segmentation quality is measured during validation (`validate_one_epoch`) by comparing its predictions against the *full ground truth masks* using standard metrics like Intersection over Union (IoU) and Accuracy.
    *   The best-performing model checkpoint is saved based on the performance achieved on the *validation set using the GT masks*, not the weak training loss value.



## 4. Results - Hybrid did not improve acc

### Single

| Model Name | Test Accuracy | IoU (Background) | IoU (Pet) | Avg IoU |
| --- | --- | --- | --- | --- |
| Points | 0.5359 | 0.1109 | 0.3394 | 0.2252 |
| Scribbles | 0.7073 | 0.5417 | 0.4473 | 0.4945 |
| Boxes | 0.7151 | 0.5881 | 0.4535 | 0.5208 |


### Hybrid
Equal Loss Weight

- Combined loss, sum over each individual loss with lambda factor = 1

| Model Name | Test Accuracy | IoU (Background) | IoU (Pet) | Avg IoU |
| --- | --- | --- | --- | --- |
| Points + Scribbles | 0.6825 | 0.4912 | 0.4472 | 0.4692 |
| Points + Boxes | 0.6954 | 0.5524 | 0.4346 | 0.4935 |
| Scribbles + Boxes | 0.7104 | 0.5689 | 0.4639 | 0.5164 |
| Points + Scribbles + Boxes | 0.6981 | 0.5573 | 0.4423 | 0.4998 |

As we see that hybrid did not improve the performance, we made an hypothesis is that the equal loss factor we set in loss, so we adapt  Uncertainty based weighting to see if adaptive weight can mitigate some of the performance drop ([https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf))

*Uncertainty-based weighting*

| Model Name | Test Accuracy | IoU (Background) | IoU (Pet) | Avg IoU |
| --- | --- | --- | --- | --- |
| Points + Scribble | 0.6829 | 0.4866 | 0.4252 | 0.4559 |
| Points + Boxes | 0.6992 | 0.5537 | 0.4373 | 0.4955 |
| Scribbles + Boxes | 0.7103 | 0.5653 | 0.4493 | 0.5073 |
| Points + Scribbles + Boxes | 0.7066 | 0.5599 | 0.4453 | 0.5026 |

We can see that both loss give the same trend of decreasing iou, so in here, we will step in to analysis what did we do right and wrong, and what concluions we can have in here

### Reasons why the experiment is valid

1. Signal Conflict
    - Boxes enforce coarse object boundaries.
    - Scribbles/Points refine interiors but lack boundary information.
    - Conflict: The model receives mixed signals (e.g., "focus on boundaries" vs. "ignore boundaries, focus on interiors"), leading to optimization instability.
2. Class Imbalance & Model Behavior
    - Background dominance: With pets occupying 29.97% of pixels (background: 70.03%), a trivial baseline labeling *everything* as background would achieve:
        - Accuracy: ~70% (matches background prevalence).
        - Avg IoU: ~35% (background IoU: 70%, pet IoU: 0%).
    - Our results:
        - Box-only model: Achieves 71.51% accuracy and 52.08% Avg IoU (Pet IoU: 45.35%, Background IoU: 58.81%).
        - This exceeds the trivial baseline, proving the model does not cheat by labeling everything as background.
        - The non-zero pet IoU (45.35%) confirms meaningful learning of pet regions.
3. Pseudo generated label contain large noise
    1. Since we used the highest form label bounding box, to generate subsequent poitns and scribble, it will result in high noise, not all points fall in the regions of true pet area
    2. Since scribbles/points are derived from boxes, their noise is bounded by box accuracy. The bigger issue is their **sparsity**, not noise.
4. Annotation Coverage Analysis

| **Annotation** | **Coverage (Pixels)** | **Spatial Consistency** |
| --- | --- | --- |
| Boxes | 81% | High |
| Scribbles | 25% | Medium |
| Points | 2% | Low |
- Result: Boxes dominate training due to superior coverage and boundary information. Scribbles/points are too sparse to meaningfully contribute.
1. Loss Dynamics
    - Box-only Loss: Stable optimization (lowest test loss: 0.5696).
    - Hybrid Loss: Higher losses (e.g., 1.2276 for box+point+scribble) indicate conflicting gradients.
