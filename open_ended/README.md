
# Open-Ended


## Research Questions
How does the type and potential combination of sparse weak annotations (specifically bounding boxes, simulated points, and simulated scribbles) influence the segmentation performance and learning characteristics of a weakly-supervised model trained on the Oxford-IIIT Pet Dataset, compared to using only image-level labels or full pixel-level supervision?

## Installation
Additional installation
```bash
pip install torchmetrics
```

## Run

```
!cd comp0197-cw2/ && python open_ended/download_data.py

!cd comp0197-cw2/ && python open_ended/weak_label_generator.py --data_dir ./data --output_file ./weak_labels/weak_labels_train.pkl


!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode points \
    --run_name segnet_points_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single \
    --augment

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode scribbles \
    --run_name segnet_scribbles_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single \
    --augment

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode boxes \
    --run_name segnet_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single \
    --augment


!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_scribbles \
    --run_name segnet_hybrid_points_scribbles_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid \
    --augment

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_boxes \
    --run_name segnet_hybrid_points_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir comp0197/checkpoints_hybrid \
    --augment

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_scribbles_boxes \
    --run_name segnet_hybrid_scribbles_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid \
    --augment

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_scribbles_boxes \
    --run_name segnet_hybrid_points_scribbles_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid \
    --augment
```

## Visualization


Visualize labels
```
python -m open_ended.visualize_labels    
```

Visualize weights
```
python -m open_ended.weight_visualization
```



## Evaluate

```
!cd comp0197-cw2/  && python evaluate.py \
    --data_dir ./data \
    --model_paths checkpoints_single/segnet_point_run1_best_acc.pth \
                  checkpoints_single/segnet_scatter_run1_best_acc.pth \
                  checkpoints_single/segnet_boxes_run1_best_acc.pth \
                  checkpoints_hybrid/segnet_hybrid_point_scatter_run1_best_acc.pth \
    --batch_size 8 \
    --device cuda
```

## Plan

**Phase 1: Setup & Data Preparation**


1.  **Weak Label Generation:** 
    *   Points (centroid of each object mask).
    *   Scribbles (freehand stroke per object and background)
    *   Bounding boxes (tightest box around mask).

The process of generating weak labels begins by iterating through a specified subset of image files and locating their corresponding trimap annotations. For each trimap, the code first loads it, records its original dimensions, and converts it into a binary foreground mask by identifying pixels assumed to represent the object (checking for a specific pixel value, like 1 or 255, which needs to match the trimap's convention). If a valid foreground mask is created, several types of weak labels are derived from it using the original image coordinates: 

- Points are generated by finding distinct connected components (objects) within the mask and randomly sampling a predefined number of pixel coordinates (x, y) from within each component. 
- Scribbles are created by first finding the morphological skeleton (a thin line representation) of the foreground mask, identifying the longest path along this skeleton, and sampling points from it for the foreground scribble; background scribbles are similarly generated from the longest skeleton path within the region outside a dilated version of the foreground mask. 
- Bounding Boxes are determined by identifying each separate foreground object and calculating the minimum and maximum x and y coordinates that enclose it (xmin, ymin, xmax, ymax). 

These generated points, scribbles, and bounding boxes, along with the original image size, are then stored together, in a dictionary keyed by the image name, and saved to a file for later use in training weakly supervised models.
    

**Phase 2: Model Training**

*   **Data Preparation:**
    *   The `PetsDataset` is configured based on the chosen `supervision_mode` (e.g., 'points', 'scribbles', 'boxes', 'hybrid\_...').
    *   For each training sample, the dataloader provides:
        1.  The input image.
        2.  The corresponding *weak labels* (e.g., point coordinates, scribble paths, bounding box coordinates, or a mix in hybrid modes).
        3.  The *full ground truth (GT) segmentation mask* (primarily used for evaluation, not the weak training loss itself).

*   **Model Prediction:**
    *   The segmentation model (`SegNetWrapper`) processes the input image batch.
    *   It outputs dense, pixel-wise predictions (logits) representing the probability of each pixel belonging to the foreground (pet) or background class.

*   **Weakly Supervised Loss Calculation:**
    *   This is the key step where weak labels are used for learning.
    *   A specialized loss function (`CombinedLoss` or `PartialCrossEntropyLoss`) is employed.
    *   This loss function compares the model's predictions *only* at the locations specified by the *weak labels*.
        *   For **points/scribbles**: The loss is calculated based on whether the model correctly predicts foreground/background *at those specific pixel coordinates*. Pixels *not* covered by the points/scribbles are typically ignored in the loss calculation (e.g., using `IGNORE_INDEX`).
        *   For **boxes**: The loss might encourage the model to predict foreground *within* the box and potentially background *outside* it, or use other box-based weak supervision techniques integrated into the `CombinedLoss`.
        *   For **hybrid modes**: The `CombinedLoss` intelligently aggregates the loss signals derived from *all available* weak label types (points, scribbles, boxes) for that specific training image.
    *   The goal is to penalize the model for incorrect predictions *at the weakly labeled locations*.

*   **Model Update:**
    *   The calculated loss value, derived *from the weak supervision signal*, is used for backpropagation.
    *   Gradients are computed, and the optimizer (`AdamW`) updates the model's weights to minimize this weak loss over time.

*   **Evaluation and Checkpointing:**
    *   **Crucially:** While training relies on the *weak loss*, the model's actual segmentation quality is measured during validation (`validate_one_epoch`) by comparing its predictions against the *full ground truth masks* using standard metrics like Intersection over Union (IoU) and Accuracy.
    *   The best-performing model checkpoint is saved based on the performance achieved on the *validation set using the GT masks*, not the weak training loss value.



## Results


### Single Feature
**SegNet**

| SegNet  | Performance  |               |           |               |
|---------|--------------|---------------|-----------|---------------|
| Feature | Best Val IOU | Best Test IOU | Test Loss | Test Accuracy |
| box     | 0.4826       | 0.5338        | 0.5696    | 0.7295        |
| Scribbles | 0.3343       | 0.3307        | 3.0825    | 0.5080        |
| point   | 0.3320       | 0.1522        | 4.5684    | 0.5000        |

### Hybrid Feature

**SegNet**

| SegNet              | Performance  |               |           |               |
|---------------------|--------------|---------------|-----------|---------------|
| Feature             | Best Val IOU | Best Test IOU | Test Loss | Test Accuracy |
| box, point          | 0.4655       | 0.4793        | 0.9793    | 0.7018        |
| box, Scribbles        | 0.4630       | 0.4737        | 1.0722    | 0.7144        |
| point, Scribbles      | 0.3444       | 0.2027        | 0.3929    | 0.5344        |
| box, point, Scribbles | 0.4694       | 0.4705        | 1.2276    | 0.7185        |



---

## Expected Results and Interesting Things to See

**Expected Quantitative Results:**

*   We expect a performance ranking roughly following the amount of spatial information provided:
    *   `Points` likely better than Tags, but potentially still quite low depending on how well the partial CE loss works without propagation.
    *   `Scribbles` and `Boxes` likely significantly better than Points/Tags, potentially achieving similar mIoU scores to each other.
    *   `Hybrid (Tags + Points)`: This is the key experimental result. We expect it to perform *better* than the `Points`-only model. The interesting question is *how much* better? Will it approach the performance of `Scribbles` or `Boxes`? It's unlikely to surpass them, but closing a significant portion of the gap would be noteworthy.
*   The cost-effectiveness plot should visually reinforce this trade-off, potentially showing diminishing returns as annotation effort increases from points/scribbles/boxes. The hybrid point will add an interesting data point to this curve.

**Expected Qualitative Results:**

*   **Points:** Might segment small regions around the click points well but struggle with object extent and boundaries. May fail completely on objects that weren't clicked (if simulating only one click per *image* instead of per *object*). Class accuracy might be reasonable if points are class-labeled.
*   **Scribbles:** Should produce more complete object shapes than points, potentially with somewhat accurate boundaries where the scribble provides guidance, but rough elsewhere. Might struggle with very thin structures not covered by scribbles.
*   **Boxes:** Likely yield fairly complete object masks but with boundaries adhering somewhat to the box shape, potentially including background pixels near corners or failing to separate objects within the same box.



No longer under experiment because of tags is not valid to compare with, but we can use the idea from this for the hybrid experiment.

*   **Hybrid (Tags + Points):**
    *   **Interesting Thing 1 (Completeness):** Compared to Points-only, we hope to see *more complete object segmentation*. Does the global tag information help the model "fill in" the object beyond the single point location?
    *   **Interesting Thing 2 (Class Consistency):** Does adding the tag loss reduce class confusion errors compared to Points-only? E.g., if a point is ambiguously placed, does the image-level tag help assign the correct class to the segmented region?
    *   **Interesting Thing 3 (Failure Modes):** Will the hybrid model exhibit failure modes that are a blend of Tag and Point failures, or does it find a genuinely better solution? For instance, does it still struggle with boundaries like the Point model, or does it become blobby like the Tag model?

>  **Overall:** The most interesting outcome relates to the **hybrid model's effectiveness**. Seeing a improvement over points-only with such minimal extra supervision (tags) would underscore the value of even coarse global context in WSSS. Conversely, if the improvement is negligible, it would suggest that for this architecture/task, simple loss combination isn't enough, and more sophisticated fusion or propagation is needed to leverage minimal signals effectively. The qualitative analysis will be crucial to understand *why* the hybrid model performs as it does.



## Exploring Hybrid Spatial Supervision

After I trained model with individual types of weak labels. I found anther interesting part with mixing them, which seems highly relevant for practical use cases and less explored in an empircal analysis from past papers.

Think about it: if a team is already annotating bounding boxes, we could automatically generate points (like centroids) and maybe even basic scribbles from those boxes. This would give us richer training data for the segmentation model without asking annotators to do more work.

To see how much benefit we actually get from this, I think the below set of experiments are good enough to see how hybrid improve/or not the result.



So, we have experimented with:
  - Points + Scribble
  - Poitns + Bounding box
  - scrible + bounding box
  - points + scribble + bounding box


All the models were trained with same settings, ex. batch size etc.


## Results

