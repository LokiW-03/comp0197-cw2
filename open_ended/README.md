
# Open-Ended


## Research Questions
How does the type and potential combination of sparse weak annotations (specifically bounding boxes, simulated points, and simulated scribbles) influence the segmentation performance and learning characteristics of a weakly-supervised model trained on the Oxford-IIIT Pet Dataset, compared to using only image-level labels or full pixel-level supervision?

## Installation
Additional installation
```bash
pip install torchmetrics pillow matplotlib
```

## Run

```
!cd comp0197-cw2/ && python open_ended/download_data.py

# We dont need to generate new labels, as labels is uploaded in git
!cd comp0197-cw2/ && python open_ended/weak_label_generator.py --data_dir ./data --output_file ./weak_labels/weak_labels_train.pkl

#Visualize pseudo labels
!cd comp0197-cw2/ && python -m open_ended.visualize_labels  --seed 19 --label_file open_ended/weak_labels/weak_labels_train.pkl --



!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode points \
    --run_name segnet_points_run1 \
    --data_dir ./data \
    --weak_label_path ./open_ended/weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode scribbles \
    --run_name segnet_scribbles_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single



!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode boxes \
    --run_name segnet_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_single


# Hybrid

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_scribbles \
    --run_name segnet_hybrid_points_scribbles_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_boxes \
    --run_name segnet_hybrid_points_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_scribbles_boxes \
    --run_name segnet_hybrid_scribbles_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid

!cd comp0197-cw2/  && python -m open_ended.train \
    --supervision_mode hybrid_points_scribbles_boxes \
    --run_name segnet_hybrid_points_scribbles_boxes_run1 \
    --data_dir ./data \
    --weak_label_path ./weak_labels/weak_labels_train.pkl \
    --batch_size 64 \
    --lr 2e-4 \
    --epochs 25 \
    --num_workers 8 \
    --img_size 256 \
    --checkpoint_dir ./checkpoints_hybrid \
    --augment
```

## Visualization


Visualize labels
```
python -m open_ended.visualize_labels    
```

Visualize weights
```
python -m open_ended.weight_visualization
```



## Evaluate

```
!cd comp0197-cw2/  && python evaluate.py \
    --data_dir ./data \
    --model_paths checkpoints_single/segnet_point_run1_best_acc.pth \
                  checkpoints_single/segnet_scatter_run1_best_acc.pth \
                  checkpoints_single/segnet_boxes_run1_best_acc.pth \
                  checkpoints_hybrid/segnet_hybrid_point_scatter_run1_best_acc.pth \
    --batch_size 8 \
    --device cuda
```

## Plan

**Phase 1: Setup & Data Preparation**


1.  **Weak Label Generation:** 
    *   Points (centroid of each object mask).
    *   Scribbles (freehand stroke per object and background)
    *   Bounding boxes (tightest box around mask).

The process of generating weak labels begins by iterating through a specified subset of image files and locating their corresponding trimap annotations. For each trimap, the code first loads it, records its original dimensions, and converts it into a binary foreground mask by identifying pixels assumed to represent the object (checking for a specific pixel value, like 1 or 255, which needs to match the trimap's convention). If a valid foreground mask is created, several types of weak labels are derived from it using the original image coordinates: 

- Points are generated by finding distinct connected components (objects) within the mask and randomly sampling a predefined number of pixel coordinates (x, y) from within each component. 
- Scribbles are created by first finding the morphological skeleton (a thin line representation) of the foreground mask, identifying the longest path along this skeleton, and sampling points from it for the foreground scribble; background scribbles are similarly generated from the longest skeleton path within the region outside a dilated version of the foreground mask. 
- Bounding Boxes are determined by identifying each separate foreground object and calculating the minimum and maximum x and y coordinates that enclose it (xmin, ymin, xmax, ymax). 

These generated points, scribbles, and bounding boxes, along with the original image size, are then stored together, in a dictionary keyed by the image name, and saved to a file for later use in training weakly supervised models.
    

**Phase 2: Model Training**

*   **Data Preparation:**
    *   The `PetsDataset` is configured based on the chosen `supervision_mode` (e.g., 'points', 'scribbles', 'boxes', 'hybrid\_...').
    *   For each training sample, the dataloader provides:
        1.  The input image.
        2.  The corresponding *weak labels* (e.g., point coordinates, scribble paths, bounding box coordinates, or a mix in hybrid modes).
        3.  The *full ground truth (GT) segmentation mask* (primarily used for evaluation, not the weak training loss itself).

*   **Model Prediction:**
    *   The segmentation model (`SegNetWrapper`) processes the input image batch.
    *   It outputs dense, pixel-wise predictions (logits) representing the probability of each pixel belonging to the foreground (pet) or background class.

*   **Weakly Supervised Loss Calculation:**
    *   This is the key step where weak labels are used for learning.
    *   A specialized loss function (`CombinedLoss` or `PartialCrossEntropyLoss`) is employed.
    *   This loss function compares the model's predictions *only* at the locations specified by the *weak labels*.
        *   For **points/scribbles**: The loss is calculated based on whether the model correctly predicts foreground/background *at those specific pixel coordinates*. Pixels *not* covered by the points/scribbles are typically ignored in the loss calculation (e.g., using `IGNORE_INDEX`).
        *   For **boxes**: The loss might encourage the model to predict foreground *within* the box and potentially background *outside* it, or use other box-based weak supervision techniques integrated into the `CombinedLoss`.
        *   For **hybrid modes**: The `CombinedLoss` intelligently aggregates the loss signals derived from *all available* weak label types (points, scribbles, boxes) for that specific training image.
    *   The goal is to penalize the model for incorrect predictions *at the weakly labeled locations*.

*   **Model Update:**
    *   The calculated loss value, derived *from the weak supervision signal*, is used for backpropagation.
    *   Gradients are computed, and the optimizer (`AdamW`) updates the model's weights to minimize this weak loss over time.

*   **Evaluation and Checkpointing:**
    *   **Crucially:** While training relies on the *weak loss*, the model's actual segmentation quality is measured during validation (`validate_one_epoch`) by comparing its predictions against the *full ground truth masks* using standard metrics like Intersection over Union (IoU) and Accuracy.
    *   The best-performing model checkpoint is saved based on the performance achieved on the *validation set using the GT masks*, not the weak training loss value.



## Results


### Single Feature (To Be Updated, new set of result will be updated shortly)
**SegNet**

| SegNet  | Performance  |               |           |               |
|---------|--------------|---------------|-----------|---------------|
| Feature | Best Val IOU | Best Test IOU | Test Loss | Test Accuracy |
| box     | 0.4826       | 0.5338        | 0.5696    | 0.7295        |
| Scribbles | 0.3343       | 0.3307        | 3.0825    | 0.5080        |
| point   | 0.3320       | 0.1522        | 4.5684    | 0.5000        |

### Hybrid Feature (To Be Updated, new set of result will be updated shortly)

**SegNet**

| SegNet              | Performance  |               |           |               |
|---------------------|--------------|---------------|-----------|---------------|
| Feature             | Best Val IOU | Best Test IOU | Test Loss | Test Accuracy |
| box, point          | 0.4655       | 0.4793        | 0.9793    | 0.7018        |
| box, Scribbles        | 0.4630       | 0.4737        | 1.0722    | 0.7144        |
| point, Scribbles      | 0.3444       | 0.2027        | 0.3929    | 0.5344        |
| box, point, Scribbles | 0.4694       | 0.4705        | 1.2276    | 0.7185        |





## Exploring Hybrid Spatial Supervision

After I trained model with individual types of weak labels. I found anther interesting part with mixing them, which seems highly relevant for practical use cases and less explored in an empircal analysis from past papers.

Think about it: if a team is already annotating bounding boxes, we could automatically generate points (like centroids) and maybe even basic scribbles from those boxes. This would give us richer training data for the segmentation model without asking annotators to do more work.

To see how much benefit we actually get from this, I think the below set of experiments are good enough to see how hybrid improve/or not the result.



So, we have experimented with:
  - Points + Scribble
  - Poitns + Bounding box
  - scrible + bounding box
  - points + scribble + bounding box


All the models were trained with same settings, ex. batch size etc.


## Results

